{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba036fff",
   "metadata": {},
   "source": [
    "# Test Audio Analyse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3cf939",
   "metadata": {},
   "source": [
    "## Version Whisper de base "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b637ead",
   "metadata": {},
   "source": [
    "- Vitesse Rapide\n",
    "- Pr√©cision moyenne\n",
    "- Utile pour test g√©n√©rale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fbeb7e",
   "metadata": {},
   "source": [
    "##### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a07f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Go ahead and introduce your sign name is Michael with a B and I've been afraid of insects mind Stop, stop, stop, where? Hmm? Where's the B? There's the B?\n",
      "Formatted transcription saved to: bee.txt\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "import os\n",
    "import re\n",
    "\n",
    "\n",
    "# Load the Whisper model\n",
    "model = whisper.load_model(\"base\")\n",
    "\n",
    "# Path to the audio file\n",
    "audio_file = \"bee.mp4\"\n",
    "\n",
    "# Check if the file exists\n",
    "if not os.path.exists(audio_file):\n",
    "\traise FileNotFoundError(f\"The file '{audio_file}' was not found. Please check the path.\")\n",
    "\n",
    "# Transcribe the audio file\n",
    "result = model.transcribe(audio_file)\n",
    "\n",
    "# Print the transcription\n",
    "print(result[\"text\"])\n",
    "\n",
    "# Split the transcription into sentences based on punctuation\n",
    "\n",
    "# Define a function to split text into sentences\n",
    "def split_into_sentences(text):\n",
    "\t# Use regex to split by sentence-ending punctuation\n",
    "\tsentences = re.split(r'(?<=[.!?]) +', text)\n",
    "\treturn sentences\n",
    "\n",
    "# Get the segments from the result\n",
    "segments = result.get(\"segments\", [])\n",
    "\n",
    "# Initialize a list to store formatted sentences\n",
    "formatted_sentences = []\n",
    "\n",
    "# Iterate through the segments to add line breaks based on silence duration\n",
    "for segment in segments:\n",
    "\ttext = segment[\"text\"].strip()\n",
    "\tif segment[\"end\"] - segment[\"start\"] > 1:  # Check if silence duration is greater than 2 seconds\n",
    "\t\tformatted_sentences.append(text + \"\\n\")\n",
    "\telse:\n",
    "\t\tformatted_sentences.append(text)\n",
    "\n",
    "# Join the sentences with line breaks\n",
    "formatted_text = \"\\n\".join(formatted_sentences)\n",
    "\n",
    "# Save the formatted transcription to a text file\n",
    "output_file = os.path.splitext(audio_file)[0] + \".txt\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "\tf.write(formatted_text)\n",
    "\n",
    "print(\"Formatted transcription saved to:\", output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a76e31",
   "metadata": {},
   "source": [
    "## Version Whisper large V3 Turbo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031bb859",
   "metadata": {},
   "source": [
    "- Vitesse lente\n",
    "- Haute pr√©cision\n",
    "- Utile pour longue vid√©o"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d40e61",
   "metadata": {},
   "source": [
    "#### V1 fonctionnelle\n",
    "- Ne marche pas sur les longues vid√©o"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce696458",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d110049c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "c:\\Users\\pc\\anaconda3\\envs\\stage\\Lib\\site-packages\\transformers\\models\\whisper\\generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Go ahead and introduce yourself. My name is Michael with a B, and I've been afraid of insects. Stop, stop, stop. Where? Hm? Where's the B? There's a B?\n",
      "Formatted transcription saved to: beeturbo.txt\n",
      "Formatted transcription saved to: beeturbo.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model_id = \"openai/whisper-large-v3-turbo\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "dataset = load_dataset(\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\")\n",
    "sample = dataset[0][\"audio\"]\n",
    "\n",
    "audio_file_turbo = \"bee.mp4\"\n",
    "resultturbo = pipe(audio_file_turbo, return_timestamps=True)\n",
    "print(resultturbo[\"text\"])\n",
    "\n",
    "# Split the transcription into sentences based on punctuation\n",
    "\n",
    "# Define a function to split text into sentences\n",
    "def split_into_sentences(text):\n",
    "\t# Use regex to split by sentence-ending punctuation\n",
    "\tsentences = re.split(r'(?<=[.!?]) +', text)\n",
    "\treturn sentences\n",
    "\n",
    "# Get the chunks from the result\n",
    "chunks = resultturbo.get(\"chunks\", [])\n",
    "\n",
    "\n",
    "# Initialize a list to store formatted sentences\n",
    "formatted_sentences = []\n",
    "\n",
    "# Iterate through the chunks to add line breaks based on silence duration\n",
    "for chunk in chunks:\n",
    "\ttext = chunk[\"text\"].strip()\n",
    "\tstart, end = chunk[\"timestamp\"]\n",
    "\tif end - start > 1:  # Check if silence duration is greater than 1 seconds\n",
    "\t\tformatted_sentences.append(text + \"\\n\")\n",
    "\telse:\n",
    "\t\tformatted_sentences.append(text)\n",
    "\n",
    "# Join the sentences with line breaks\n",
    "formatted_text = \"\\n\".join(formatted_sentences)\n",
    "\n",
    "# Save the formatted transcription to a text file\n",
    "output_file = os.path.splitext(audio_file_turbo)[0] + \"turbo.txt\"\n",
    "if formatted_text.strip():\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(formatted_text)\t\n",
    "    print(\"Formatted transcription saved to:\", output_file)\n",
    "else:\n",
    "    print(\"Warning: Formatted text is empty. Nothing was written.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a1a975",
   "metadata": {},
   "source": [
    "#### V2 Am√©lior√©\n",
    "- Marche mieux pour les vid√©o longues\n",
    "- Met plus de temps que le mod√®le basique (40 min pour une vid√©o de 25min)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c68fba",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4ae92e",
   "metadata": {},
   "source": [
    "##### Version simple audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddaf673c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " √áa, en vrai, vraiment, vraiment √ßa. Je te le dis. Bref, mets-la en vas-y, c'est bon. Voil√†, il y a 20 mots, t'as rien dit. Comment c'est possible d'√©crire autant et de rien dire ? Voil√†, c'est...\n",
      "Formatted transcription saved to: audios\\riendireturbo.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model_id = \"openai/whisper-large-v3-turbo\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "dataset = load_dataset(\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\")\n",
    "sample = dataset[0][\"audio\"]\n",
    "\n",
    "input_dir = \"audios\"\n",
    "audio_file_turbo = os.path.join(input_dir, \"riendire.mp4\")\n",
    "resultturbo = pipe(audio_file_turbo, return_timestamps=True)\n",
    "print(resultturbo[\"text\"])\n",
    "\n",
    "# Split the transcription into sentences based on punctuation\n",
    "\n",
    "# Define a function to split text into sentences\n",
    "def split_into_sentences(text):\n",
    "\t# Use regex to split by sentence-ending punctuation\n",
    "\tsentences = re.split(r'(?<=[.!?]) +', text)\n",
    "\treturn sentences\n",
    "\n",
    "# Get the chunks from the result\n",
    "chunks = resultturbo.get(\"chunks\", [])\n",
    "\n",
    "\n",
    "# Initialize a list to store formatted sentences\n",
    "formatted_sentences = []\n",
    "\n",
    "# Iterate through the chunks to add line breaks based on silence duration\n",
    "for chunk in chunks:\n",
    "    text = chunk[\"text\"].strip()\n",
    "    start, end = chunk.get(\"timestamp\", (None, None))\n",
    "    if start is not None and end is not None and end - start > 1:  # Check if silence duration is greater than 1 second\n",
    "        formatted_sentences.append(text + \"\\n\")\n",
    "    else:\n",
    "        formatted_sentences.append(text)\n",
    "\n",
    "# Join the sentences with line breaks\n",
    "formatted_text = \"\\n\".join(formatted_sentences)\n",
    "\n",
    "# Save the formatted transcription to a text file\n",
    "output_file = os.path.splitext(audio_file_turbo)[0] + \"turbo.txt\"\n",
    "if formatted_text.strip():\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(formatted_text)\t\n",
    "    print(\"Formatted transcription saved to:\", output_file)\n",
    "else:\n",
    "    print(\"Warning: Formatted text is empty. Nothing was written.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a768b3e0",
   "metadata": {},
   "source": [
    "##### Version simple fichier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d8d58da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement du mod√®le...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üó£Ô∏è Transcription de : bee.mp4 ...\n",
      "‚úÖ Transcription enregistr√©e : audios\\bee_transcription.txt\n",
      "\n",
      "üó£Ô∏è Transcription de : Floor.mp4 ...\n",
      "‚úÖ Transcription enregistr√©e : audios\\Floor_transcription.txt\n",
      "\n",
      "üó£Ô∏è Transcription de : Perenoel.mp4 ...\n",
      "‚úÖ Transcription enregistr√©e : audios\\Perenoel_transcription.txt\n",
      "\n",
      "üó£Ô∏è Transcription de : plion_dream.wav ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Transcription enregistr√©e : audios\\plion_dream_transcription.txt\n",
      "\n",
      "üó£Ô∏è Transcription de : riendire.mp4 ...\n",
      "‚úÖ Transcription enregistr√©e : audios\\riendire_transcription.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "\n",
    "def transcribe_folder_with_whisper(input_dir=\"audios\", extension_filter=(\".mp3\", \".wav\", \".mp4\", \".flac\")):\n",
    "    \"\"\"\n",
    "    Transcrit tous les fichiers audio dans un dossier donn√© √† l'aide de Whisper v3 Turbo.\n",
    "    \n",
    "    Args:\n",
    "        input_dir (str): Dossier contenant les fichiers audio.\n",
    "        extension_filter (tuple): Extensions de fichiers √† traiter.\n",
    "    \"\"\"\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "    model_id = \"openai/whisper-large-v3-turbo\"\n",
    "\n",
    "    print(\"Chargement du mod√®le...\")\n",
    "    model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "        model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    "    ).to(device)\n",
    "    \n",
    "    processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "    pipe = pipeline(\n",
    "        \"automatic-speech-recognition\",\n",
    "        model=model,\n",
    "        tokenizer=processor.tokenizer,\n",
    "        feature_extractor=processor.feature_extractor,\n",
    "        torch_dtype=torch_dtype,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    audio_files = [f for f in os.listdir(input_dir) if f.lower().endswith(extension_filter)]\n",
    "    if not audio_files:\n",
    "        print(\"Aucun fichier audio trouv√© dans le dossier sp√©cifi√©.\")\n",
    "        return\n",
    "\n",
    "    for file in audio_files:\n",
    "        file_path = os.path.join(input_dir, file)\n",
    "        print(f\"\\nüó£Ô∏è Transcription de : {file} ...\")\n",
    "\n",
    "        result = pipe(file_path, return_timestamps=True)\n",
    "        text = result.get(\"text\", \"\").strip()\n",
    "        chunks = result.get(\"chunks\", [])\n",
    "\n",
    "        # Formater avec sauts de ligne apr√®s pauses longues\n",
    "        formatted_sentences = []\n",
    "        for chunk in chunks:\n",
    "            chunk_text = chunk[\"text\"].strip()\n",
    "            start, end = chunk.get(\"timestamp\", (None, None))\n",
    "            if start is not None and end is not None and end - start > 1:\n",
    "                formatted_sentences.append(chunk_text + \"\\n\")\n",
    "            else:\n",
    "                formatted_sentences.append(chunk_text)\n",
    "        \n",
    "        formatted_text = \"\\n\".join(formatted_sentences)\n",
    "        output_file = os.path.splitext(file_path)[0] + \"_transcription.txt\"\n",
    "        \n",
    "        if formatted_text:\n",
    "            with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(formatted_text)\n",
    "            print(f\"‚úÖ Transcription enregistr√©e : {output_file}\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Aucun texte d√©tect√© pour : {file}\")\n",
    "\n",
    "# Exemple d'utilisation :\n",
    "transcribe_folder_with_whisper(\"audios\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119b8669",
   "metadata": {},
   "source": [
    "## Version Nvidia Parakeet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab4031d",
   "metadata": {},
   "source": [
    "- Audio en mono uniquement\n",
    "- Pas tr√®s rapide\n",
    "- Pas tr√®s performant\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37931c31",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db6159d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-09 14:47:13 nemo_logging:393] Tokenizer SentencePieceTokenizer initialized with 1024 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-05-09 14:47:14 nemo_logging:405] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    use_lhotse: true\n",
      "    skip_missing_manifest_entries: true\n",
      "    input_cfg: null\n",
      "    tarred_audio_filepaths: null\n",
      "    manifest_filepath: null\n",
      "    sample_rate: 16000\n",
      "    shuffle: true\n",
      "    num_workers: 2\n",
      "    pin_memory: true\n",
      "    max_duration: 40.0\n",
      "    min_duration: 0.1\n",
      "    text_field: answer\n",
      "    batch_duration: null\n",
      "    use_bucketing: true\n",
      "    bucket_duration_bins: null\n",
      "    bucket_batch_size: null\n",
      "    num_buckets: 30\n",
      "    bucket_buffer_size: 20000\n",
      "    shuffle_buffer_size: 10000\n",
      "    \n",
      "[NeMo W 2025-05-09 14:47:14 nemo_logging:405] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    use_lhotse: true\n",
      "    manifest_filepath: null\n",
      "    sample_rate: 16000\n",
      "    batch_size: 16\n",
      "    shuffle: false\n",
      "    max_duration: 40.0\n",
      "    min_duration: 0.1\n",
      "    num_workers: 2\n",
      "    pin_memory: true\n",
      "    text_field: answer\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-09 14:47:14 nemo_logging:393] PADDING: 0\n",
      "[NeMo I 2025-05-09 14:47:19 nemo_logging:393] Using RNNT Loss : tdt\n",
      "    Loss tdt_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0, 'durations': [0, 1, 2, 3, 4], 'sigma': 0.02, 'omega': 0.1}\n",
      "[NeMo I 2025-05-09 14:47:19 nemo_logging:393] Using RNNT Loss : tdt\n",
      "    Loss tdt_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0, 'durations': [0, 1, 2, 3, 4], 'sigma': 0.02, 'omega': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-05-09 14:47:19 nemo_logging:405] No conditional node support for Cuda.\n",
      "    Cuda graphs with while loops are disabled, decoding speed will be slower\n",
      "    Reason: CUDA is not available\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-09 14:47:19 nemo_logging:393] Using RNNT Loss : tdt\n",
      "    Loss tdt_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0, 'durations': [0, 1, 2, 3, 4], 'sigma': 0.02, 'omega': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-05-09 14:47:19 nemo_logging:405] No conditional node support for Cuda.\n",
      "    Cuda graphs with while loops are disabled, decoding speed will be slower\n",
      "    Reason: CUDA is not available\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-09 14:47:21 nemo_logging:393] Model EncDecRNNTBPEModel was successfully restored from C:\\Users\\pc\\.cache\\huggingface\\hub\\models--nvidia--parakeet-tdt-0.6b-v2\\snapshots\\50aec6a056e85b9f95b612df08a2bddc55b58714\\parakeet-tdt-0.6b-v2.nemo.\n",
      "[NeMo I 2025-05-09 14:47:22 nemo_logging:393] Timestamps requested, setting decoding timestamps to True. Capture them in Hypothesis object,                         with output[0][idx].timestep['word'/'segment'/'char']\n",
      "[NeMo I 2025-05-09 14:47:22 nemo_logging:393] Using RNNT Loss : tdt\n",
      "    Loss tdt_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0, 'durations': [0, 1, 2, 3, 4], 'sigma': 0.02, 'omega': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-05-09 14:47:22 nemo_logging:405] `include_duration` is not implemented for CUDA graphs\n",
      "Transcribing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go ahead and introduce yourself. My name is Michael with a bee and I've been afraid of insects mindset. Top stops out. Where? Hmm? Where's the bee? Where's the bee?\n",
      "Segment timestamps are not available in the output.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the ASR module from NeMo\n",
    "import nemo.collections.asr as nemo_asr\n",
    "import logging\n",
    "\n",
    "logging.getLogger('nemo').setLevel(logging.WARNING)\n",
    "\n",
    "\n",
    "\n",
    "# Load the ASR model\n",
    "asr_model = nemo_asr.models.ASRModel.from_pretrained(model_name=\"nvidia/parakeet-tdt-0.6b-v2\")\n",
    "\n",
    "from pydub import AudioSegment\n",
    "\n",
    "# Path to the audio file\n",
    "audio_file = \"bee.mp4\"\n",
    "\n",
    "# Convert the audio file to mono and save it as a temporary file\n",
    "audio = AudioSegment.from_file(audio_file)\n",
    "audio = audio.set_channels(1)  # Convert to mono\n",
    "temp_audio_file = \"temp_mono_audio.wav\"\n",
    "audio.export(temp_audio_file, format=\"wav\")\n",
    "\n",
    "# Transcribe the audio file (ensure audio is in the correct format)\n",
    "output = asr_model.transcribe([temp_audio_file], timestamps=True)\n",
    "\n",
    "# Print transcribed text\n",
    "print(output[0].text)  # Access the 'text' attribute of the Hypothesis object\n",
    "\n",
    "# Extract segment timestamps (start and end times for each segment)\n",
    "if hasattr(output[0], \"word_timestamps\"):\n",
    "    word_timestamps = output[0].word_timestamps  # Word-level timestamps\n",
    "\n",
    "    # Print word timestamps\n",
    "    for word in word_timestamps:\n",
    "        print(f\"{word['start']}s - {word['end']}s : {word['word']}\")  # Access attributes of the word dictionary\n",
    "else:\n",
    "    print(\"Segment timestamps are not available in the output.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stage",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
